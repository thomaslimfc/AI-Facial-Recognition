{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pip install notebook\n",
    "\n",
    "# pip install opencv-python-headless torch torchvision pillow yolov8"
   ],
   "id": "9b81dfe17d963c58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Facial Recognition System with YOLOv8, AgeNet, and GenderNet",
   "id": "709a406f34780ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T03:34:12.041539Z",
     "start_time": "2024-08-05T03:34:12.009731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from yolov8 import YOLOv8\n",
    "\n",
    "# Initialize models\n",
    "yolo_model = YOLOv8('yolov8s.pt')\n",
    "age_model = torch.load('agenet.pth')\n",
    "gender_model = torch.load('gendernet.pth')\n",
    "\n",
    "# Define preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Setup complete.\")"
   ],
   "id": "3571c0f78d0cdd98",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models, transforms\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Object Detection with YOLOv8",
   "id": "9c1d40a0ad8d1fce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:56:11.912137Z",
     "start_time": "2024-08-03T18:56:11.725738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_objects(frame):\n",
    "    results = yolo_model(frame)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    detections = detect_objects(frame)\n",
    "    # Process detections here...\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "8481be63dd2caf90",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m cap \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241m.\u001B[39mVideoCapture(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m      8\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Age Detection with AgeNet",
   "id": "51eb1a99b94167d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:56:14.428396Z",
     "start_time": "2024-08-03T18:56:14.424685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_age(face_image):\n",
    "    face_tensor = transform(face_image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        age_prediction = age_model(face_tensor)\n",
    "    return age_prediction\n",
    "\n",
    "# Example usage\n",
    "# age = predict_age(face_image)\n"
   ],
   "id": "eed05c0a6653389",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Gender Detection with GenderNet",
   "id": "8241b2c26155065e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:56:16.914517Z",
     "start_time": "2024-08-03T18:56:16.910954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_gender(face_image):\n",
    "    face_tensor = transform(face_image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        gender_prediction = gender_model(face_tensor)\n",
    "    return gender_prediction\n",
    "\n",
    "# Example usage\n",
    "# gender = predict_gender(face_image)\n"
   ],
   "id": "d1ec8bc3f8212104",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Combining Everything",
   "id": "ca8f6054901fb86a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:56:20.220192Z",
     "start_time": "2024-08-03T18:56:20.198711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_frame(frame):\n",
    "    detections = detect_objects(frame)\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, label = detection\n",
    "        if label == 'face':\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            face_image = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            age_prediction = predict_age(face_image)\n",
    "            gender_prediction = predict_gender(face_image)\n",
    "\n",
    "            # Display results\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Age: {age_prediction}\", (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Gender: {gender_prediction}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Facial Recognition System', frame)\n",
    "\n",
    "# Example usage\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    process_frame(frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "f26091c35b4e5eba",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m     cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFacial Recognition System\u001B[39m\u001B[38;5;124m'\u001B[39m, frame)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m cap \u001B[38;5;241m=\u001B[39m \u001B[43mcv2\u001B[49m\u001B[38;5;241m.\u001B[39mVideoCapture(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "67160b5dc3875e83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
